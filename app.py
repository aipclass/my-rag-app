import streamlit as st
import os
import arxiv
from dotenv import load_dotenv
import requests

# --- ç›´æ¥ç”¨å®˜æ–¹ SDKï¼ˆzhipuai>=2.xï¼‰ï¼Œä¸å†ä¾èµ– langchain_glm ä»¥å‡å°‘å®‰è£…é—®é¢˜ ---
try:
    from zhipuai import ZhipuAI  # type: ignore
except Exception:
    ZhipuAI = None

# --- LangChain æ ¸å¿ƒç»„ä»¶ ---
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import FastEmbedEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from typing import List, Optional, Any

# åŠ è½½ç¯å¢ƒå˜é‡ (åœ¨Streamlit Cloudä¸Šä¼šè‡ªåŠ¨è¯»å–Secrets)
load_dotenv()


# --- 0. æœ€å°HF Inference APIå°è£…ï¼šä½œä¸ºæ™ºè°±ä¸å¯ç”¨æ—¶çš„è‡ªåŠ¨å›é€€ ---
class HfInferenceLLM(LLM):
    repo_id: str
    temperature: float = 0.3
    max_new_tokens: int = 2048
    timeout: float = 60.0
    fallback_models: Optional[List[str]] = None

    @property
    def _llm_type(self) -> str:
        return "hf-inference-api"

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
        if not token:
            raise RuntimeError("ç¼ºå°‘ HUGGINGFACEHUB_API_TOKENï¼Œè¯·åœ¨Secretsä¸­é…ç½®ã€‚")

        url = f"https://api-inference.huggingface.co/models/{self.repo_id}"
        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
        payload = {
            "inputs": prompt,
            "parameters": {
                "temperature": self.temperature,
                "max_new_tokens": self.max_new_tokens,
                "return_full_text": False,
            },
            "options": {"wait_for_model": True},
        }

        resp = requests.post(url, headers=headers, json=payload, timeout=self.timeout)
        if resp.status_code == 404:
            fallback_env = os.getenv("HF_FALLBACK_MODELS")
            fallbacks = (
                [m.strip() for m in fallback_env.split(",") if m.strip()]
                if fallback_env
                else (self.fallback_models or [
                    "google/flan-t5-base",
                    "google/flan-t5-small",
                    "google/flan-t5-large",
                ])
            )
            for fb in fallbacks:
                fb_url = f"https://api-inference.huggingface.co/models/{fb}"
                fb_resp = requests.post(fb_url, headers=headers, json=payload, timeout=self.timeout)
                if fb_resp.ok:
                    self.repo_id = fb
                    try:
                        data = fb_resp.json()
                    except Exception:
                        return fb_resp.text
                    if isinstance(data, list) and data and isinstance(data[0], dict):
                        text = data[0].get("generated_text")
                        if text is not None:
                            return text
                    if isinstance(data, dict) and "generated_text" in data:
                        return str(data["generated_text"])  # type: ignore
                    return str(data)
            raise RuntimeError("HF Inference API 404ï¼šé€‰å®šä¸å›é€€æ¨¡å‹å‡ä¸å¯ç”¨ã€‚")

        if resp.status_code == 429:
            raise RuntimeError("HF Inference API 429ï¼šé¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚")

        resp.raise_for_status()
        try:
            data = resp.json()
        except Exception:
            return resp.text
        if isinstance(data, list) and data and isinstance(data[0], dict):
            text = data[0].get("generated_text")
            if text is not None:
                return text
        if isinstance(data, dict):
            if "generated_text" in data:
                return str(data["generated_text"])  # type: ignore
            if "error" in data:
                raise RuntimeError(f"HF Inference API error: {data['error']}")
        return str(data)


# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="AIè®ºæ–‡æœç´¢ä¸é—®ç­”æœºå™¨äºº", page_icon=" C", layout="wide")
st.title(" C AIè®ºæ–‡æœç´¢ä¸é—®ç­”æœºå™¨äºº")
st.write("åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥æœç´¢arXivä¸Šçš„è®ºæ–‡ï¼Œå¹¶ä¸é€‰å®šçš„è®ºæ–‡è¿›è¡Œæ™ºèƒ½å¯¹è¯ã€‚")

# --- 2. å®šä¹‰è·¯å¾„ ---
PDF_SAVE_PATH = "downloaded_papers"
if not os.path.exists(PDF_SAVE_PATH):
    os.makedirs(PDF_SAVE_PATH)


# --- 3. ç¼“å­˜çš„æ•°æ®å¤„ç†å‡½æ•° (æ ¸å¿ƒåŠŸèƒ½ä¸å˜) ---
@st.cache_resource
def get_retriever_and_metadata(_paper_id):
    """
    ä¸‹è½½è®ºæ–‡PDFï¼ŒåŠ è½½ã€åˆ‡åˆ†ã€å‘é‡åŒ–ï¼Œå¹¶åˆ›å»ºæ£€ç´¢å™¨ã€‚
    åˆ©ç”¨Streamlitçš„ç¼“å­˜é¿å…é‡å¤è®¡ç®—ã€‚
    """
    print(f"--- [Cache Miss] æ­£åœ¨ä¸ºè®ºæ–‡ {_paper_id} æ„å»ºæ£€ç´¢å™¨ ---")
    client = arxiv.Client()
    search = arxiv.Search(id_list=[_paper_id])
    paper = next(client.results(search))

    # ä½¿ç”¨å¥å£®çš„æ–¹å¼è·å– arXiv çŸ­IDï¼Œå…¼å®¹æ—§å¼IDï¼ˆå¦‚ "cs/0506025"ï¼‰ä¸æ–°å¼ID
    try:
        short_id = paper.get_short_id()  # type: ignore[attr-defined]
    except Exception:
        short_id = paper.entry_id.split('abs/')[-1]
    pdf_filename = f"{short_id}.pdf"
    local_pdf_path = os.path.join(PDF_SAVE_PATH, pdf_filename)

    # å¦‚æœæœ¬åœ°ä¸å­˜åœ¨PDFï¼Œåˆ™ä¸‹è½½
    if not os.path.exists(local_pdf_path):
        paper.download_pdf(dirpath=PDF_SAVE_PATH, filename=pdf_filename)

    # 1. åŠ è½½æ–‡æ¡£
    loader = PyMuPDFLoader(local_pdf_path)
    docs = loader.load()

    # 2. åˆ‡åˆ†æ–‡æ¡£
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # 3. å‘é‡åŒ–å¹¶åˆ›å»ºFAISSç´¢å¼•ï¼ˆæ”¹ä¸º fastembedï¼Œé¿å…å®‰è£…å¤§å‹PyTorch ä¾èµ–ï¼‰
    embeddings = FastEmbedEmbeddings()
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

    # 4. åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(search_kwargs={'k': 6})

    return retriever, paper, local_pdf_path


# --- Session State åˆå§‹åŒ– ---
if 'stage' not in st.session_state:
    st.session_state.stage = 'search'
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", return_messages=True, output_key='answer'
    )

# --- é€‰æ‹©è®ºæ–‡çš„å›è°ƒï¼Œé¿å…å¾ªç¯å˜é‡ç»‘å®šé—®é¢˜ï¼Œç¡®ä¿ç‚¹å‡»å“ªä¸€é¡¹å°±è¿›å…¥å“ªä¸€é¡¹ ---
def _on_select_paper(paper_id: str) -> None:
    # é€‰æ‹©æ–°è®ºæ–‡æ—¶æ¸…ç†ç¼“å­˜ï¼Œé¿å…å› ç¼“å­˜é”®å¼‚å¸¸å¯¼è‡´å§‹ç»ˆå±•ç¤ºåŒä¸€ç¯‡è®ºæ–‡
    try:
        get_retriever_and_metadata.clear()
    except Exception:
        pass
    st.session_state.selected_paper_id = paper_id
    st.session_state.stage = 'chat'
    st.session_state.messages = []
    st.session_state.memory.clear()

# --- åº”ç”¨æµç¨‹æ§åˆ¶ ---

# ================= é˜¶æ®µ1: æœç´¢è®ºæ–‡ =================
if st.session_state.stage == 'search':
    st.header("1. æœç´¢è®ºæ–‡")
    query = st.text_input("è¾“å…¥æ‚¨æƒ³æœç´¢çš„è®ºæ–‡å…³é”®è¯", key="search_query")
    if st.button(" C æœç´¢"):
        if query:
            with st.spinner("æ­£åœ¨arXivä¸Šæœç´¢..."):
                client = arxiv.Client()
                search = arxiv.Search(query=query, max_results=5, sort_by=arxiv.SortCriterion.Relevance)
                results = list(client.results(search))
                if results:
                    st.session_state.search_results = results
                    st.session_state.stage = 'select'
                    st.rerun()
                else:
                    st.error("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è®ºæ–‡ï¼Œè¯·æ¢ä¸ªå…³é”®è¯è¯•è¯•ã€‚")
        else:
            st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ã€‚")

# ================= é˜¶æ®µ2: é€‰æ‹©è®ºæ–‡ =================
elif st.session_state.stage == 'select':
    st.header("2. é€‰æ‹©ä¸€ç¯‡è®ºæ–‡è¿›è¡Œå¯¹è¯")
    if 'search_results' in st.session_state and st.session_state.search_results:
        # ä½¿ç”¨å¯é¢„æµ‹çš„ key ä¸ on_click å›è°ƒï¼Œé¿å… for å¾ªç¯é—­åŒ…å¯¼è‡´å§‹ç»ˆå–ç¬¬ä¸€é¡¹
        for idx, paper in enumerate(st.session_state.search_results):
            # å…¼å®¹æ—§å¼ä¸æ–°å¼ arXiv IDï¼Œé¿å…å› ä¸¢å¤±åˆ†ç±»å‰ç¼€å¯¼è‡´å§‹ç»ˆæ‰“å¼€åŒä¸€ç¯‡è®ºæ–‡
            try:
                paper_id = paper.get_short_id()  # type: ignore[attr-defined]
            except Exception:
                paper_id = paper.entry_id.split('abs/')[-1]
            st.subheader(paper.title)
            st.write(f"**ä½œè€…**: {', '.join(author.name for author in paper.authors)}")
            st.write(f"**æ‘˜è¦**: {paper.summary[:300]}...")
            st.button(
                " C ä¸è¿™ç¯‡è®ºæ–‡å¯¹è¯",
                key=f"select_btn_{idx}_{paper_id}",
                on_click=_on_select_paper,
                kwargs={"paper_id": paper_id},
            )
    if st.button("è¿”å›æœç´¢"):
        st.session_state.pop('search_results', None)
        st.session_state.stage = 'search'
        st.rerun()

# ================= é˜¶æ®µ3: ä¸è®ºæ–‡å¯¹è¯ =================
elif st.session_state.stage == 'chat':
    paper_id = st.session_state.get('selected_paper_id')
    if not paper_id:
        st.warning("æœªæ£€æµ‹åˆ°å·²é€‰æ‹©çš„è®ºæ–‡ï¼Œè¯·å…ˆè¿”å›åˆ—è¡¨é‡æ–°é€‰æ‹©ã€‚")
        if st.button("è¿”å›è®ºæ–‡åˆ—è¡¨"):
            st.session_state.stage = 'select'
            st.rerun()
        st.stop()

    try:
        retriever, paper_metadata, downloaded_pdf_path = get_retriever_and_metadata(paper_id)

        # --- ä¼˜å…ˆä½¿ç”¨æ™ºè°±AIï¼ˆå®˜æ–¹ SDK + ç®€å•é€‚é…å™¨ï¼‰ï¼›è‹¥ä¸å¯ç”¨åˆ™å›é€€ HF å…è´¹æ¨¡å‹ ---
        llm = None
        current_model_label = ""
        zhipuai_api_key = os.getenv("ZHIPUAI_API_KEY")
        zhipu_model = os.getenv("ZHIPU_MODEL", "glm-4-flash")
        if ZhipuAI is not None and zhipuai_api_key:
            class ZhipuLLMAdapter(LLM):
                model: str
                api_key: str
                temperature: float = 0.3

                @property
                def _llm_type(self) -> str:
                    return "zhipu-adapter"

                def _call(self, prompt: str, stop=None, run_manager=None, **kwargs):
                    client = ZhipuAI(api_key=self.api_key)
                    resp = client.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.temperature,
                    )
                    # å…¼å®¹ SDK è¿”å›
                    try:
                        return resp.choices[0].message.content  # type: ignore
                    except Exception:
                        return str(resp)


            llm = ZhipuLLMAdapter(model=zhipu_model, api_key=zhipuai_api_key, temperature=0.3)
            current_model_label = f"ZhipuAI {zhipu_model}"
        if llm is None:
            # å›é€€ï¼šä½¿ç”¨HFå…è´¹æ¨¡å‹
            selected_model = os.getenv("HF_MODEL_ID", "google/flan-t5-base")
            llm = HfInferenceLLM(
                repo_id=selected_model,
                temperature=0.3,
                max_new_tokens=512,
                fallback_models=[
                    "google/flan-t5-base",
                    "google/flan-t5-small",
                    "google/flan-t5-large",
                ]
            )
            current_model_label = f"HF {selected_model}"

        # åˆ›å»ºå¯¹è¯æ£€ç´¢é“¾ (åç»­é€»è¾‘ä¸å˜)
        rag_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=st.session_state.memory,
            combine_docs_chain_kwargs={
                "prompt": PromptTemplate(
                    template="""[ä»»åŠ¡æŒ‡ä»¤]
                    ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„AIå­¦æœ¯ç ”ç©¶å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸‹æ–¹æä¾›çš„â€œ[è®ºæ–‡ç›¸å…³å†…å®¹]â€ï¼Œä»¥ä¸€ç§æ·±åˆ»ã€ä¸“ä¸šä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„å£å»ï¼Œè¯¦ç»†å›ç­”ç”¨æˆ·çš„â€œ[é—®é¢˜]â€ã€‚
                    [çŸ¥è¯†èŒƒå›´]: ä½ çš„æ‰€æœ‰å›ç­”å¿…é¡»ä¸¥æ ¼æ¥æºäºä¸‹æ–¹æä¾›çš„â€œ[è®ºæ–‡ç›¸å…³å†…å®¹]â€ã€‚ç»å¯¹ç¦æ­¢ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–è¿›è¡Œæ— æ ¹æ®çš„çŒœæµ‹ã€‚
                    [çº¦æŸæ¡ä»¶]: å¦‚æœå†…å®¹ç‰‡æ®µç¡®å®æ— æ³•æ”¯æ’‘å›ç­”ï¼Œå°±ç›´æˆªäº†å½“åœ°è¯´ï¼šâ€œè¿™ç¯‡è®ºæ–‡çš„ç›¸å…³éƒ¨åˆ†æœªè®¨è®ºæ­¤é—®é¢˜ã€‚â€
                    ---
                    [è®ºæ–‡ç›¸å…³å†…å®¹]: {context}
                    ---
                    [é—®é¢˜]: {question}
                    [ä½ çš„ä¸“å®¶çº§åˆ†æå›ç­”]:
                    """,
                    input_variables=["context", "question"]
                )
            },
            return_source_documents=True
        )

        # --- èŠå¤©ç•Œé¢ ---
        st.header(f"3. æ­£åœ¨ä¸è®ºæ–‡å¯¹è¯: {paper_metadata.title}")

        # æ˜¾ç¤ºå½“å‰å®é™…ä½¿ç”¨çš„æ¨¡å‹
        st.caption(f"å½“å‰æ¨¡å‹: {current_model_label}")

        with open(downloaded_pdf_path, "rb") as pdf_file:
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½å½“å‰è®ºæ–‡PDF",
                data=pdf_file,
                file_name=os.path.basename(downloaded_pdf_path),
                mime="application/octet-stream"
            )
        st.divider()

        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # æ¥æ”¶ç”¨æˆ·è¾“å…¥
        user_question = st.chat_input("è¯·å°±è¿™ç¯‡è®ºæ–‡æé—®ï¼š")
        if user_question:
            st.session_state.messages.append({"role": "user", "content": user_question})
            with st.chat_message("user"):
                st.markdown(user_question)

            # è°ƒç”¨RAGé“¾è·å–å›ç­”
            with st.spinner("æ¨¡å‹æ­£åœ¨æ£€ç´¢ä¸æ€è€ƒä¸­..."):
                result = rag_chain.invoke({"question": user_question})
                ai_response = result['answer']

                st.session_state.messages.append({"role": "assistant", "content": ai_response})

                # æ˜¾ç¤ºAIå›ç­”å’Œå¼•ç”¨çš„åŸæ–‡
                with st.chat_message("assistant"):
                    st.markdown(ai_response)
                    with st.expander("æŸ¥çœ‹æœ¬æ¬¡å›ç­”å¼•ç”¨çš„åŸæ–‡ç‰‡æ®µ"):
                        for doc in result.get('source_documents', []):
                            st.markdown(
                                f"> {doc.page_content}\n\n_(æ¥æº: PDF ç¬¬ {doc.metadata.get('page', 'N/A')} é¡µ)_")

        # è¿”å›æŒ‰é’®
        if st.button(" C è¿”å›è®ºæ–‡é€‰æ‹©åˆ—è¡¨"):
            st.session_state.stage = 'select'
            st.session_state.messages = []
            st.session_state.memory.clear()
            st.session_state.pop('selected_paper_id', None)
            st.rerun()

    except Exception as e:
        st.error(f"å¤„ç†å¯¹è¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        if st.button("è¿”å›é‡è¯•"):
            # æ¸…ç†ç¼“å­˜å¹¶é‡è¯•
            get_retriever_and_metadata.clear()
            st.rerun()
