import streamlit as st
import os
import arxiv
# --- ã€ä¿®æ”¹ç‚¹ 1ã€‘: å¯¼å…¥æ–°çš„HuggingFaceHubå’Œç¯å¢ƒå˜é‡åŠ è½½åº“ ---
from langchain_community.llms import HuggingFaceHub
from dotenv import load_dotenv

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# --- éƒ¨ç½²å‡†å¤‡: åŠ è½½ç¯å¢ƒå˜é‡ (æœ¬åœ°æµ‹è¯•ç”¨) ---
# åœ¨æœ¬åœ°å¼€å‘æ—¶ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ª .env æ–‡ä»¶å­˜æ”¾HUGGINGFACEHUB_API_TOKEN
# åœ¨Streamlit Cloudéƒ¨ç½²æ—¶ï¼Œåˆ™éœ€è¦é…ç½®Secrets
load_dotenv()

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="arXiv RAG é—®ç­”æœºå™¨äºº (äº‘ç«¯ç‰ˆ)", page_icon="â˜ï¸", layout="wide")
st.title("â˜ï¸ arXiv RAG é—®ç­”æœºå™¨äºº (äº‘ç«¯ç‰ˆ)")
st.write("æœ¬åº”ç”¨ç»“åˆäº‘ç«¯Qwenæ¨¡å‹APIä¸RAGæŠ€æœ¯ï¼Œå…è®¸æ‚¨ä¸æŒ‡å®šçš„arXivè®ºæ–‡è¿›è¡Œå¤šè½®å¯¹è¯ã€‚")

# --- 2. æ–‡ä»¶å¤¹è·¯å¾„å®šä¹‰ ---
PDF_SAVE_PATH = "./downloaded_papers"
if not os.path.exists(PDF_SAVE_PATH):
    os.makedirs(PDF_SAVE_PATH)


# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (ç¼“å­˜ä»¥æé«˜æ€§èƒ½) ---
@st.cache_resource
def setup_pipelines(paper_id="2307.09288"):
    print("--- æ­£åœ¨æ‰§è¡Œä¸€æ¬¡æ€§åˆå§‹åŒ– (æ­¤éƒ¨åˆ†å°†è¢«ç¼“å­˜) ---")

    # --- ä¸‹è½½è®ºæ–‡éƒ¨åˆ† ---
    search = arxiv.Search(id_list=[paper_id])
    paper = next(search.results())
    pdf_filename = f"{paper.entry_id.split('/')[-1]}.pdf"
    local_pdf_path = os.path.join(PDF_SAVE_PATH, pdf_filename)
    if not os.path.exists(local_pdf_path):
        paper.download_pdf(dirpath=PDF_SAVE_PATH, filename=pdf_filename)

    # --- åµŒå…¥ä¸æ–‡æ¡£å¤„ç† ---
    embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
    embeddings = SentenceTransformerEmbeddings(model_name=embedding_model_name)

    loader = PyMuPDFLoader(local_pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

    # --- ã€ä¿®æ­£ç‚¹ã€‘: å°†åˆ›å»ºretrieverçš„ä»£ç åˆå¹¶ä¸ºæ­£ç¡®çš„ä¸€è¡Œ ---
    retriever = vectorstore.as_retriever(search_kwargs={'k': 6})

    # --- LLMè°ƒç”¨éƒ¨åˆ† ---
    repo_id = "Qwen/Qwen1.5-7B-Chat"
    llm = HuggingFaceHub(
        repo_id=repo_id,
        model_kwargs={"temperature": 0.3, "max_length": 2048}
    )

    # --- Promptå’ŒChainçš„å…¶ä½™éƒ¨åˆ† ---
    qa_template = """[ä»»åŠ¡æŒ‡ä»¤]
    ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„AIå­¦æœ¯ç ”ç©¶å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸‹æ–¹æä¾›çš„â€œ[è®ºæ–‡ç›¸å…³å†…å®¹]â€ï¼Œä»¥ä¸€ç§æ·±åˆ»ã€ä¸“ä¸šä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„å£å»ï¼Œè¯¦ç»†å›ç­”ç”¨æˆ·çš„â€œ[é—®é¢˜]â€ã€‚

    [è§’è‰²æ‰®æ¼”è¦æ±‚]
    - **èº«ä»½**: ä½ å·²ç»ç²¾è¯»å¹¶å®Œå…¨ç†è§£äº†è¿™ç¯‡è®ºæ–‡ã€‚ä½ çš„å›ç­”åº”è¯¥ä½“ç°å‡ºè¿™ä¸€ç‚¹ï¼Œé¿å…ä½¿ç”¨â€œæ ¹æ®æä¾›çš„èµ„æ–™...â€è¿™ç±»æ‹‰å¼€è·ç¦»çš„è¡¨è¿°ã€‚
    - **è¯­æ°”**: ä¸“ä¸šã€è‡ªä¿¡ã€åˆ†ææ€§å¼ºã€‚åƒä¸€ä¸ªçœŸæ­£çš„ä¸“å®¶åœ¨è¿›è¡Œå­¦æœ¯äº¤æµã€‚
    - **çŸ¥è¯†èŒƒå›´**: ä½ çš„æ‰€æœ‰å›ç­”å¿…é¡»ä¸¥æ ¼æ¥æºäºä¸‹æ–¹æä¾›çš„â€œ[è®ºæ–‡ç›¸å…³å†…å®¹]â€ã€‚ç»å¯¹ç¦æ­¢ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–è¿›è¡Œæ— æ ¹æ®çš„çŒœæµ‹ã€‚

    [å›ç­”æ„å»ºæŒ‡å—]
    1.  **å¯¹äºæ€»ç»“/åˆ†æç±»é—®é¢˜**: è¯·ç»¼åˆæ‰€æœ‰ç›¸å…³å†…å®¹ç‰‡æ®µï¼Œæç‚¼å‡ºæ ¸å¿ƒè®ºç‚¹ã€æ–¹æ³•å’Œç»“è®ºï¼Œå¹¶ç”¨è‡ªå·±çš„è¯è¿›è¡Œæœ‰æ¡ç†çš„ç»„ç»‡å’Œé˜è¿°ã€‚
    2.  **å¯¹äºäº‹å®æŸ¥è¯¢ç±»é—®é¢˜**: è¯·ç›´æ¥ä»å†…å®¹ä¸­å®šä½å¹¶æŠ½å–å‡ºå…·ä½“çš„æ•°å­—ã€åç§°æˆ–äº‹å®ï¼Œå¹¶æ¸…æ™°åœ°å‘ˆç°å‡ºæ¥ã€‚
    3.  **çº¦æŸæ¡ä»¶**: å¦‚æœå†…å®¹ç‰‡æ®µç¡®å®æ— æ³•æ”¯æ’‘å›ç­”ï¼Œå°±ç›´æˆªäº†å½“åœ°è¯´ï¼šâ€œè¿™ç¯‡è®ºæ–‡çš„ç›¸å…³éƒ¨åˆ†æœªè®¨è®ºæ­¤é—®é¢˜ã€‚â€

    ---
    [è®ºæ–‡ç›¸å…³å†…å®¹]:
    {context}
    ---

    [é—®é¢˜]: {question}

    [ä½ çš„ä¸“å®¶çº§åˆ†æå›ç­”]:
    """
    QA_PROMPT = PromptTemplate.from_template(qa_template)

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key='answer'
    )

    rag_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": QA_PROMPT},
        return_source_documents=True,
        verbose=True
    )

    print("--- åˆå§‹åŒ–å®Œæˆ ---")
    return rag_chain, paper, local_pdf_path

# --- ã€ä¿®æ”¹ç‚¹ 3ã€‘: ä½¿ç”¨Streamlit Secretsè·å–APIä»¤ç‰Œ ---
# æ£€æŸ¥APIä»¤ç‰Œæ˜¯å¦å­˜åœ¨ï¼Œæä¾›æ›´å‹å¥½çš„æç¤º
if 'HUGGINGFACEHUB_API_TOKEN' not in os.environ and 'HUGGINGFACEHUB_API_TOKEN' not in st.secrets:
    st.error("éƒ¨ç½²é”™è¯¯ï¼šHugging Face APIä»¤ç‰Œæœªé…ç½®ï¼è¯·åœ¨Streamlit Cloudçš„Secretsä¸­è®¾ç½®å®ƒã€‚")
    st.image("https://i.imgur.com/i2uCIgV.png")  # æ·»åŠ ä¸€ä¸ªç¤ºä¾‹å›¾ç‰‡æŒ‡å¯¼ç”¨æˆ·
    st.stop()

# --- ä¸»ç¨‹åºå’Œç•Œé¢æ˜¾ç¤ºä»£ç ä¿æŒä¸å˜ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.spinner("æ­£åœ¨åˆå§‹åŒ–é—®ç­”æœºå™¨äººï¼Œé¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´..."):
    try:
        rag_chain, paper_metadata, downloaded_pdf_path = setup_pipelines()
        st.success("åˆå§‹åŒ–å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ã€‚")
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {e}")
        st.stop()

st.subheader("å½“å‰è®ºæ–‡ä¿¡æ¯")
st.markdown(
    f"**æ ‡é¢˜:** {paper_metadata.title}\n\n**ä½œè€…:** {', '.join(author.name for author in paper_metadata.authors)}\n\n**å‘å¸ƒæ—¥æœŸ:** {paper_metadata.published.strftime('%Y-%m-%d')}")
with open(downloaded_pdf_path, "rb") as pdf_file:
    st.download_button(label="ğŸ“¥ ä¸‹è½½è®ºæ–‡PDF", data=pdf_file, file_name=os.path.basename(downloaded_pdf_path),
                       mime="application/octet-stream")
st.divider()

st.subheader("å¯¹è¯è®°å½•")
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_question = st.chat_input("è¯·å°±è¿™ç¯‡è®ºæ–‡æé—®ï¼š")

if user_question:
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    with st.spinner("æ¨¡å‹æ­£åœ¨æ£€ç´¢ä¸æ€è€ƒä¸­..."):
        try:
            result = rag_chain({"question": user_question})
            ai_response = result['answer']

            st.session_state.messages.append({"role": "assistant", "content": ai_response})
            with st.chat_message("assistant"):
                st.markdown(ai_response)
                with st.expander("æŸ¥çœ‹æœ¬æ¬¡å›ç­”å¼•ç”¨çš„åŸæ–‡ç‰‡æ®µ"):
                    if 'source_documents' in result and result['source_documents']:
                        for i, doc in enumerate(result['source_documents']):
                            st.markdown(
                                f"**ç‰‡æ®µ {i + 1} (æ¥è‡ª PDF ç¬¬ {doc.metadata.get('page', 'N/A')} é¡µ):**\n> {doc.page_content}")
                    else:
                        st.write("è­¦å‘Šï¼šæœ¬æ¬¡å›ç­”æœªèƒ½ä»è®ºæ–‡ä¸­æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³çš„åŸæ–‡ç‰‡æ®µã€‚")
        except Exception as e:
            st.error(f"è·å–å›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {e}")